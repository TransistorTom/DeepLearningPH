/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Traceback (most recent call last):
  File "/home2/s3306801/github/DeepLearningPH/SLURM_scripts/pipeline_slurm.py", line 33, in <module>
    model, train_df, test_dfs, history_loss = pipeline(
  File "/home2/s3306801/github/DeepLearningPH/functions/pipeline.py", line 48, in pipeline
    model, loss_history = train_model(model, train_data, epochs=epochs, batch_size=batch_size, lr=lr)
  File "/home2/s3306801/github/DeepLearningPH/functions/train_model.py", line 64, in train_model
    for data in train_loader:
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1480, in _next_data
    return self._process_data(data)
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1505, in _process_data
    data.reraise()
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/_utils.py", line 733, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py", line 27, in __call__
    return Batch.from_data_list(
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch_geometric/data/batch.py", line 97, in from_data_list
    batch, slice_dict, inc_dict = collate(
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch_geometric/data/collate.py", line 109, in collate
    value, slices, incs = _collate(attr, values, data_list, stores,
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch_geometric/data/collate.py", line 192, in _collate
    storage = elem.untyped_storage()._new_shared(
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/storage.py", line 411, in _new_shared
    return cls(size, device=device)
  File "/home2/s3306801/envs/DLP/lib/python3.10/site-packages/torch/cuda/__init__.py", line 305, in _lazy_init
    raise RuntimeError(
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method

